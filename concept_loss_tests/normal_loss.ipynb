{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10**4\n",
    "num_concepts = 10\n",
    "num_latent_concepts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from independent normals using parameterization trick\n",
    "def sample(num_concepts, num_latent_concepts, batch_size):\n",
    "    mu_hat = np.random.random(num_concepts).reshape(num_concepts, 1) # (n_concepts,  1)\n",
    "    true_sigma_hat = np.random.random(num_concepts).reshape(num_concepts, 1) # (n_concepts,  1)\n",
    "    c_hat = np.random.standard_normal(batch_size*num_concepts).reshape(batch_size, num_concepts, 1) * true_sigma_hat**0.5 + mu_hat # (batch_size, n_concepts,  1)\n",
    "\n",
    "    mu_tilde = np.random.random(num_latent_concepts).reshape(num_latent_concepts, 1) # (n_latent,  1)\n",
    "    true_sigma_tilde = np.random.random(num_latent_concepts).reshape(num_latent_concepts, 1) # (n_latent,  1)\n",
    "    c_tilde = np.random.standard_normal(batch_size*num_latent_concepts).reshape(batch_size, num_latent_concepts, 1) * true_sigma_tilde**0.5 + mu_tilde # (batch_size, n_latent,  1)\n",
    "    # print(list(true_sigma_hat.squeeze())+list(true_sigma_tilde.squeeze()))\n",
    "    return c_hat, c_tilde\n",
    "\n",
    "# c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "# c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "# print(c_hat.shape, c_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "num_concepts = 10\n",
    "num_latent_concepts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.50801948  0.34928405 -1.23892489 -0.11905367  2.14910639  0.40555758\n",
      "   0.31533124  1.64649657 -0.43002407  0.70916933 -0.63043551]\n",
      " [ 0.34928405  3.88679952 -0.72615895 -0.13777566 -0.86369586 -0.01565253\n",
      "  -0.21964978  1.45482667  0.45473369  0.28833711 -1.66158596]\n",
      " [-1.23892489 -0.72615895  1.09417509  0.00714445 -0.76532203  0.0194465\n",
      "  -0.09876495 -0.76703291  0.74452219 -0.2413108   0.23643475]\n",
      " [-0.11905367 -0.13777566  0.00714445  2.89089595  1.14839562 -0.10447844\n",
      "  -0.32334484  0.3292649   0.57830816  1.53864522  0.33501168]\n",
      " [ 2.14910639 -0.86369586 -0.76532203  1.14839562  5.80118135 -0.18919827\n",
      "  -0.17522312  2.24642314 -0.83515468  2.47589484  2.699616  ]\n",
      " [ 0.40555758 -0.01565253  0.0194465  -0.10447844 -0.18919827  0.1296983\n",
      "   0.03599095  0.06759967 -0.02476861  0.30425554 -0.25990725]\n",
      " [ 0.31533124 -0.21964978 -0.09876495 -0.32334484 -0.17522312  0.03599095\n",
      "   0.15628472 -0.11467802 -0.07175104 -0.35736829 -0.12685467]\n",
      " [ 1.64649657  1.45482667 -0.76703291  0.3292649   2.24642314  0.06759967\n",
      "  -0.11467802  1.97763408 -0.33641938  1.84785846  0.21662176]\n",
      " [-0.43002407  0.45473369  0.74452219  0.57830816 -0.83515468 -0.02476861\n",
      "  -0.07175104 -0.33641938  1.89294019 -0.22619311 -0.43694914]\n",
      " [ 0.70916933  0.28833711 -0.2413108   1.53864522  2.47589484  0.30425554\n",
      "  -0.35736829  1.84785846 -0.22619311  5.20220465  0.02580843]\n",
      " [-0.63043551 -1.66158596  0.23643475  0.33501168  2.699616   -0.25990725\n",
      "  -0.12685467  0.21662176 -0.43694914  0.02580843  2.70883022]]\n",
      "\n",
      "[[ 5.50801948  0.34928405 -1.23892489 -0.11905367  2.14910639  0.40555758\n",
      "   0.31533124  1.64649657 -0.43002407  0.70916933 -0.63043551]\n",
      " [ 0.34928405  3.88679952 -0.72615895 -0.13777566 -0.86369586 -0.01565253\n",
      "  -0.21964978  1.45482667  0.45473369  0.28833711 -1.66158596]\n",
      " [-1.23892489 -0.72615895  1.09417509  0.00714445 -0.76532203  0.0194465\n",
      "  -0.09876495 -0.76703291  0.74452219 -0.2413108   0.23643475]\n",
      " [-0.11905367 -0.13777566  0.00714445  2.89089595  1.14839562 -0.10447844\n",
      "  -0.32334484  0.3292649   0.57830816  1.53864522  0.33501168]\n",
      " [ 2.14910639 -0.86369586 -0.76532203  1.14839562  5.80118135 -0.18919827\n",
      "  -0.17522312  2.24642314 -0.83515468  2.47589484  2.699616  ]\n",
      " [ 0.40555758 -0.01565253  0.0194465  -0.10447844 -0.18919827  0.1296983\n",
      "   0.03599095  0.06759967 -0.02476861  0.30425554 -0.25990725]\n",
      " [ 0.31533124 -0.21964978 -0.09876495 -0.32334484 -0.17522312  0.03599095\n",
      "   0.15628472 -0.11467802 -0.07175104 -0.35736829 -0.12685467]\n",
      " [ 1.64649657  1.45482667 -0.76703291  0.3292649   2.24642314  0.06759967\n",
      "  -0.11467802  1.97763408 -0.33641938  1.84785846  0.21662176]\n",
      " [-0.43002407  0.45473369  0.74452219  0.57830816 -0.83515468 -0.02476861\n",
      "  -0.07175104 -0.33641938  1.89294019 -0.22619311 -0.43694914]\n",
      " [ 0.70916933  0.28833711 -0.2413108   1.53864522  2.47589484  0.30425554\n",
      "  -0.35736829  1.84785846 -0.22619311  5.20220465  0.02580843]\n",
      " [-0.63043551 -1.66158596  0.23643475  0.33501168  2.699616   -0.25990725\n",
      "  -0.12685467  0.21662176 -0.43694914  0.02580843  2.70883022]]\n"
     ]
    }
   ],
   "source": [
    "def covariance_short(c_hat, c_tilde):\n",
    "    c_hat = c_hat.squeeze(axis=-1).T\n",
    "    c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "    samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "    \n",
    "    samples_mean = np.mean(samples, axis=1)\n",
    "    exey = np.outer(samples_mean, samples_mean)\n",
    "    exy = np.zeros(exey.shape)\n",
    "    cov_again = np.zeros(exey.shape)\n",
    "    for i in range(exey.shape[0]):\n",
    "        mean_row_i = np.mean(samples[i])\n",
    "        sample_mean_row_i = np.sum(samples[i]) / (len(samples[i])-1)\n",
    "        for j in range(exey.shape[1]):\n",
    "            mean_row_j = np.mean(samples[j])\n",
    "            sample_mean_row_j = np.sum(samples[j]) / (len(samples[j])-1)\n",
    "            # cov_again[i,j] = np.mean(samples[i]*samples[j]) - mean_row_i*mean_row_j\n",
    "            cov_again[i,j] = np.sum((samples[i]-mean_row_i) * (samples[j]-mean_row_j)) / (len(samples[j])-1)\n",
    "            # cov_again[i,j] = np.sum( samples[i]*samples[j] - samples[i]*mean_row_j - mean_row_i*samples[j] + mean_row_i*mean_row_j ) / (len(samples[j])-1)\n",
    "            # cov_again[i,j] = np.sum( samples[i]*samples[j] ) / (len(samples[j])-1) - sample_mean_row_i*mean_row_j - mean_row_i*sample_mean_row_j + mean_row_i*mean_row_j\n",
    "            # cov_again[i,j] = np.sum(samples[i]*samples[j])/(samples.shape[1]) - mean_row_i*mean_row_j\n",
    "            exy[i,j] = np.mean(samples[i]*samples[j])\n",
    "    # print(exy.shape)\n",
    "    # exy_true = np.cov(samples) + exey\n",
    "\n",
    "    # print(exy)\n",
    "    # print()\n",
    "    # print(exy_true)\n",
    "\n",
    "\n",
    "    cov = exy - exey\n",
    "    return cov_again\n",
    "\n",
    "def covariance_np(c_hat, c_tilde):\n",
    "    c_hat = c_hat.squeeze(axis=-1).T\n",
    "    c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "    samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "    \n",
    "    return np.cov(samples)\n",
    "\n",
    "\n",
    "    # print(samples_mean.shape)\n",
    "    # cov = \n",
    "\n",
    "c_hat, c_tilde = sample(num_concepts, num_latent_concepts, batch_size)\n",
    "sigma = covariance(c_hat, c_tilde)\n",
    "sigma_np = covariance_np(c_hat, c_tilde)\n",
    "print(sigma)\n",
    "print()\n",
    "print(sigma_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(c_hat, c_tilde):\n",
    "    c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "    c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "    c_stack_T = np.concatenate((c_hat,c_tilde), axis=1) # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_hat_mean_resize = np.repeat(c_hat_mean.reshape(1,num_concepts,1), batch_size, axis=0) # (batch_size, n_concepts, 1)\n",
    "    c_tilde_mean_resize = np.repeat(c_tilde_mean.reshape(1,num_latent_concepts,1), batch_size, axis=0) # (batch_size, n_latent, 1)\n",
    "    c_stack_mean_T = np.concatenate((c_hat_mean_resize,c_tilde_mean_resize), axis=1)  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation = c_stack_T-c_stack_mean_T  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation_T = np.moveaxis(c_stack_deviation,-1,-2)  # (batch_size, 1, n_concepts+n_latent)\n",
    "    sigma = 1/(batch_size-1) * np.sum(np.matmul(c_stack_deviation,c_stack_deviation_T),axis=0)  # (batch_size, n_concepts+n_latent, n_concepts+n_latent)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8135731125215057, 0.7576167103399972, 0.31160523050571065, 0.8604086080584533]\n",
      "[[ 0.67459205 -0.415014    0.00300902 -0.02950988]\n",
      " [-0.415014    1.25191919 -0.17795501 -0.46840892]\n",
      " [ 0.00300902 -0.17795501  0.27022026  0.0027806 ]\n",
      " [-0.02950988 -0.46840892  0.0027806   0.86797543]]\n",
      "\n",
      "[[ 0.60713285 -0.3735126   0.00270811 -0.02655889]\n",
      " [-0.3735126   1.12672727 -0.16015951 -0.42156803]\n",
      " [ 0.00270811 -0.16015951  0.24319824  0.00250254]\n",
      " [-0.02655889 -0.42156803  0.00250254  0.78117789]]\n",
      "0.5258061794321258\n",
      "0.09648715386070177 0.06330522164800642\n"
     ]
    }
   ],
   "source": [
    "c_hat, c_tilde = sample(num_concepts, num_latent_concepts, batch_size)\n",
    "sigma_scratch = covariance(c_hat, c_tilde)\n",
    "c_hat = c_hat.squeeze(axis=-1).T\n",
    "c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "sigma_np = np.cov(samples)\n",
    "print(sigma_np)\n",
    "print()\n",
    "print(sigma_scratch)\n",
    "print(np.sum(np.abs(sigma_scratch - sigma_np)))\n",
    "print(np.linalg.det(sigma_np), np.linalg.det(sigma_scratch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_np(c_hat, c_tilde):\n",
    "    c_hat = c_hat.squeeze(axis=-1).T\n",
    "    c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "    samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "    sigma = np.cov(samples)\n",
    "    # print(sigma)\n",
    "    det_sigma = np.linalg.det(sigma)\n",
    "    # print(det_sigma)\n",
    "    log_det_sigma = np.log(det_sigma)\n",
    "\n",
    "    # print(samples[c_hat.shape[0]:, c_hat.shape[0]:].shape)\n",
    "    sigma_tilde = np.cov(samples[c_hat.shape[0]:, c_hat.shape[0]:])\n",
    "    det_sigma_tilde = np.linalg.det(sigma_tilde)\n",
    "    log_det_sigma_tilde = np.log(det_sigma_tilde) # scalar\n",
    "    r = (log_det_sigma - log_det_sigma_tilde) / 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(c_hat, c_tilde):\n",
    "    c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "    c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "    c_stack_T = np.concatenate((c_hat,c_tilde), axis=1) # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_hat_mean_resize = np.repeat(c_hat_mean.reshape(1,num_concepts,1), batch_size, axis=0) # (batch_size, n_concepts, 1)\n",
    "    c_tilde_mean_resize = np.repeat(c_tilde_mean.reshape(1,num_latent_concepts,1), batch_size, axis=0) # (batch_size, n_latent, 1)\n",
    "    c_stack_mean_T = np.concatenate((c_hat_mean_resize,c_tilde_mean_resize), axis=1)  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation = c_stack_T-c_stack_mean_T  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation_T = np.moveaxis(c_stack_deviation,-1,-2)  # (batch_size, 1, n_concepts+n_latent)\n",
    "    sigma = 1/(batch_size-1) * np.sum(np.matmul(c_stack_deviation,c_stack_deviation_T),axis=0)  # (batch_size, n_concepts+n_latent, n_concepts+n_latent)\n",
    "    return sigma\n",
    "\n",
    "def loss(c_hat, c_tilde, covariance):\n",
    "    # c_hat = c_hat.squeeze(axis=-1).T\n",
    "    # c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "    # samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "    sigma = covariance(c_hat, c_tilde)\n",
    "    # print(sigma)\n",
    "    det_sigma = np.linalg.det(sigma)\n",
    "    # print(det_sigma)\n",
    "    log_det_sigma = np.log(det_sigma)\n",
    "\n",
    "    # print(samples[c_hat.shape[0]:, c_hat.shape[0]:].shape)\n",
    "    sigma_tilde = sigma[c_hat.shape[1]:, c_hat.shape[1]:]\n",
    "    det_sigma_tilde = np.linalg.det(sigma_tilde)\n",
    "    log_det_sigma_tilde = np.log(det_sigma_tilde) # scalar\n",
    "    r = (log_det_sigma - log_det_sigma_tilde) / 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_term(c_hat, c_tilde):\n",
    "    c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "    c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "\n",
    "    c_stack_T = np.concatenate((c_hat,c_tilde), axis=1) # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_hat_mean_resize = np.repeat(c_hat_mean.reshape(1,num_concepts,1), batch_size, axis=0) # (batch_size, n_concepts, 1)\n",
    "    c_tilde_mean_resize = np.repeat(c_tilde_mean.reshape(1,num_latent_concepts,1), batch_size, axis=0) # (batch_size, n_latent, 1)\n",
    "    c_stack_mean_T = np.concatenate((c_hat_mean_resize,c_tilde_mean_resize), axis=1)  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation = c_stack_T-c_stack_mean_T  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation_T = np.moveaxis(c_stack_deviation,-1,-2)  # (batch_size, 1, n_concepts+n_latent)\n",
    "    sigma = 1/(batch_size-1) * np.sum(np.matmul(c_stack_deviation,c_stack_deviation_T),axis=0)  # (batch_size, n_concepts+n_latent, n_concepts+n_latent)\n",
    "    # sigma = covariance(c_hat, c_tilde)\n",
    "    print(sigma.shape)\n",
    "    log_det_sigma = np.log(np.linalg.det(sigma)) # scalar\n",
    "\n",
    "    c_tilde_deviation = c_tilde-c_tilde_mean # (batch_size, n_latent, 1)\n",
    "    c_tilde_deviation_T = np.moveaxis(c_tilde_deviation,-1,-2) # (batch_size, 1, n_latent)\n",
    "    sigma_tilde = 1/(batch_size-1) * np.sum(np.matmul(c_tilde_deviation, c_tilde_deviation_T),axis=0)  # (n_latent, n_latent)\n",
    "    # sigma_tilde = sigma[c_hat.shape[1]:, c_hat.shape[1]:]\n",
    "    log_det_sigma_tilde = np.log(np.linalg.det(sigma_tilde)) # scalar\n",
    "\n",
    "    r = (log_det_sigma - log_det_sigma_tilde) / 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1304/3746537734.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "  log_det_sigma = np.log(det_sigma)\n",
      "/tmp/ipykernel_1304/3746537734.py:26: RuntimeWarning: divide by zero encountered in log\n",
      "  log_det_sigma_tilde = np.log(det_sigma_tilde) # scalar\n",
      "/tmp/ipykernel_1304/3746537734.py:27: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  r = (log_det_sigma - log_det_sigma_tilde) / 2\n",
      "/tmp/ipykernel_1304/1979655526.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  log_det_sigma = np.log(np.linalg.det(sigma)) # scalar\n",
      "/tmp/ipykernel_1304/1979655526.py:20: RuntimeWarning: divide by zero encountered in log\n",
      "  log_det_sigma_tilde = np.log(np.linalg.det(sigma_tilde)) # scalar\n",
      "/tmp/ipykernel_1304/1979655526.py:22: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  r = (log_det_sigma - log_det_sigma_tilde) / 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n",
      "(132, 132)\n",
      "nan nan\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_concepts = 7\n",
    "num_latent_concepts = 4\n",
    "\n",
    "for _ in range(100):\n",
    "    c_hat, c_tilde = sample(num_concepts, num_latent_concepts, batch_size)\n",
    "    # r = loss_term(c_hat, c_tilde)\n",
    "    # r_fast = loss_np(c_hat, c_tilde)\n",
    "    r = loss(c_hat, c_tilde, covariance_np)\n",
    "    # r_fast = loss(c_hat, c_tilde, covariance)\n",
    "    r_fast = loss_term(c_hat, c_tilde)\n",
    "    # print(r_fast)\n",
    "    # if np.isnan(r_fast):\n",
    "    #     break\n",
    "    print(r, r_fast)\n",
    "    # print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5875920078389916\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 64, 64) (64, 64) (64, 64)\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_concepts = 256\n",
    "num_latent_concepts = 128\n",
    "\n",
    "c_hat, c_tilde = sample(num_concepts, num_latent_concepts, batch_size*128)\n",
    "# c = np.concatenate((c_hat, c_tilde), axis=1).squeeze().reshape(128, batch_size, num_concepts+num_latent_concepts)\n",
    "c = c_hat.reshape(128, batch_size, num_concepts)\n",
    "# # WRONG\n",
    "# c_cov = []\n",
    "# for i in range(c.shape[0]):\n",
    "#     c_cov.append(np.cov(c[i], rowvar=True))\n",
    "# c_cov = np.stack(c_cov, axis=0)\n",
    "# c_cov_mean = np.mean(c_cov, axis=0)\n",
    "# c_cov_std = np.std(c_cov, axis=0)\n",
    "# print(c_cov.shape, c_cov_mean.shape, c_cov_std.shape)\n",
    "# print(np.sum(c_cov_mean > 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from independent normals using parameterization trick\n",
    "def sample(num_concepts, num_latent_concepts, batch_size):\n",
    "    mu_hat = np.random.random(num_concepts).reshape(num_concepts, 1)*2 # (n_concepts,  1)\n",
    "    true_sigma_hat = np.random.random(num_concepts).reshape(num_concepts, 1)*5 # (n_concepts,  1)\n",
    "    c_hat = np.random.standard_normal(batch_size*num_concepts).reshape(batch_size, num_concepts, 1) * true_sigma_hat**0.5 + mu_hat # (batch_size, n_concepts,  1)\n",
    "\n",
    "    mu_tilde = np.random.random(num_latent_concepts).reshape(num_latent_concepts, 1)*5 # (n_latent,  1)\n",
    "    true_sigma_tilde = np.random.random(num_latent_concepts).reshape(num_latent_concepts, 1)*10 # (n_latent,  1)\n",
    "    c_tilde = np.random.standard_normal(batch_size*num_latent_concepts).reshape(batch_size, num_latent_concepts, 1) * true_sigma_tilde**0.5 + mu_tilde # (batch_size, n_latent,  1)\n",
    "    # print(list(true_sigma_hat.squeeze())+list(true_sigma_tilde.squeeze()))\n",
    "    return c_hat, c_tilde\n",
    "\n",
    "# c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "# c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "# print(c_hat.shape, c_tilde.shape)\n",
    "\n",
    "# sampling from independent normals using parameterization trick\n",
    "def sample_c(num_concepts, batch_size):\n",
    "    mu_hat = np.random.random(num_concepts).reshape(num_concepts)*2 # (n_concepts,  1)\n",
    "    true_sigma_hat = np.random.random(num_concepts*num_concepts).reshape(num_concepts, num_concepts)\n",
    "    true_sigma_hat = np.matmul(true_sigma_hat, true_sigma_hat.T)*5 # (n_concepts,  1)\n",
    "    c_hat = np.random.multivariate_normal(mu_hat, true_sigma_hat, batch_size)\n",
    "    return c_hat, mu_hat, true_sigma_hat\n",
    "\n",
    "# c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "# c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "# print(c_hat.shape, c_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1410018950545684\n"
     ]
    }
   ],
   "source": [
    "def get_corr_to_std(batch_size, std=1, mean=0, samples=100):\n",
    "    min_corr = -1\n",
    "    max_corr = 1\n",
    "    step = 0.01\n",
    "    scale = 100\n",
    "    corr_to_std = {}\n",
    "\n",
    "    for corr in range(int(scale*min_corr), int((max_corr+step)*scale), int(step*scale)):\n",
    "        corr /= scale\n",
    "        cov_matrix = np.ones((2,2))\n",
    "        np.fill_diagonal(cov_matrix, std**2)\n",
    "        cov_matrix[0,1] = corr*std**2\n",
    "        cov_matrix[1,0] = corr*std**2\n",
    "        z = np.random.multivariate_normal(np.ones(2)*mean, cov_matrix, (samples, batch_size))\n",
    "        corr_z = []\n",
    "        for sample in z:\n",
    "            # print(sample.shape)\n",
    "            # print(np.corrcoef(sample, rowvar=False))\n",
    "            corr_z.append(np.corrcoef(sample, rowvar=False)[0,1])\n",
    "        std_corr_z = np.std(corr_z)\n",
    "        corr_to_std[int(corr*scale)] = std_corr_z\n",
    "\n",
    "        def get_std(correlation): # interpolation\n",
    "            return corr_to_std[int(round(correlation, 2)*scale)]\n",
    "            # if correlation == 1:\n",
    "            #     return corr_to_std[correlation]\n",
    "            # if correlation < step:\n",
    "            #     return corr_to_std[0]*correlation*100 + corr_to_std[int(step*scale)]*(1-correlation*100)\n",
    "            # corr_truncate = int(correlation * scale) / scale\n",
    "            # alpha = int(correlation*scale*100) % int(corr_truncate*scale*100) / 100 # 0.1234 -> 0.34   (1234%1200)\n",
    "            # return corr_to_std[int(corr_truncate*scale)]*alpha + corr_to_std[int((corr_truncate+step)*scale)]*(1-alpha)\n",
    "    return get_std\n",
    "\n",
    "print(get_corr_to_std(batch_size)(0.8160156425812756))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "gamma: 2 149.34170877816973 24.552049911716285\n",
      "alpha: 0.0 0.0 24.552049911716285\n",
      "alpha: 0.1 14.934170877816962 24.552049911716285\n",
      "alpha: 0.2 29.868341755633924 24.552049911716285\n",
      "0.1 2\n",
      "84.53586801180836 625.736967331119 701.4422959321189\n",
      "117.23117516515754 625.736967331119 740.4640436834289\n",
      "SlogdetResult(sign=1.0, logabsdet=2171.4133422523823)\n",
      "SlogdetResult(sign=1.0, logabsdet=1439.6220783076865)\n",
      "SlogdetResult(sign=1.0, logabsdet=-10942.043162644797)\n"
     ]
    }
   ],
   "source": [
    "num_concepts = 500\n",
    "batch_size = 64\n",
    "c_hat, mu_hat, true_sigma_hat = sample_c(num_concepts, batch_size)\n",
    "get_std = np.vectorize(get_corr_to_std(batch_size))\n",
    "def get_cov(c, get_std, delta=1):\n",
    "    concepts = c.shape[1]\n",
    "    # c = np.concatenate((c_hat, c_tilde), axis=1).squeeze() # 64, 256+128\n",
    "    p_hat = np.corrcoef(c, rowvar=False) # 256+128, 256+128\n",
    "    np.fill_diagonal(p_hat, 0.99999)\n",
    "\n",
    "    # calculate S_p\n",
    "    S_p = np.sum(get_std(p_hat)**2)**0.5\n",
    "    \n",
    "\n",
    "    # estimate gamma\n",
    "    for gamma in range(2, 10, 2):\n",
    "        p_hat_gamma = p_hat**gamma * p_hat\n",
    "        print('gamma:', gamma, np.linalg.norm(p_hat - p_hat_gamma), delta * S_p)\n",
    "        if np.linalg.norm(p_hat - p_hat_gamma) >= delta * S_p:\n",
    "            gamma_star = gamma\n",
    "            break\n",
    "\n",
    "    # estimate alpha\n",
    "    scale = 1000\n",
    "    step = 0.1\n",
    "    for alpha in range(0, int((1+step)*scale), int(step*scale)):\n",
    "        alpha /= scale\n",
    "        L_alpha = alpha * p_hat ** gamma_star + (1-alpha) *p_hat ** (gamma_star-2)\n",
    "        p_hat_alpha = L_alpha * p_hat\n",
    "        print('alpha:', alpha, np.linalg.norm(p_hat - p_hat_alpha), delta * S_p)\n",
    "        if np.linalg.norm(p_hat - p_hat_alpha) >= delta * S_p:\n",
    "            alpha_star = alpha - step\n",
    "            break\n",
    "    print(alpha_star, gamma_star)\n",
    "\n",
    "    L_alpha_star = alpha_star * p_hat ** gamma_star + (1-alpha_star) *p_hat ** (gamma_star-2)\n",
    "    p_hat_nice = L_alpha_star * p_hat\n",
    "\n",
    "    V_hat = np.diagflat(np.std(c, axis=0))\n",
    "    P_hat_nice = np.matmul(np.matmul(V_hat, p_hat_nice), V_hat) # Covariance estimate\n",
    "    return P_hat_nice\n",
    "\n",
    "print(type(c_hat))\n",
    "sigma_hat = get_cov(c_hat, get_std)\n",
    "np_sigma = np.cov(c_hat, rowvar=False)\n",
    "# print(np_sigma.shape)\n",
    "# print(true_sigma_hat)\n",
    "# print()\n",
    "# print(sigma_hat)\n",
    "print(np.mean(np.absolute(true_sigma_hat-sigma_hat)), np.mean(np.absolute(true_sigma_hat)), np.mean(np.absolute(sigma_hat)))\n",
    "\n",
    "print(np.mean(np.absolute(true_sigma_hat-np_sigma)), np.mean(np.absolute(true_sigma_hat)), np.mean(np.absolute(np_sigma)))\n",
    "print(np.linalg.slogdet(true_sigma_hat))\n",
    "print(np.linalg.slogdet(sigma_hat))\n",
    "print(np.linalg.slogdet(np_sigma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[832.55805317 671.5235633  637.38296223 ... 599.1144783  618.77690238\n",
      "  603.01423257]\n",
      " [671.5235633  873.46720694 658.22410253 ... 625.33437358 637.6742321\n",
      "  638.26175424]\n",
      " [637.38296223 658.22410253 851.4920577  ... 620.80055349 627.63159767\n",
      "  617.57867465]\n",
      " ...\n",
      " [599.1144783  625.33437358 620.80055349 ... 776.92348255 587.69406409\n",
      "  609.85106122]\n",
      " [618.77690238 637.6742321  627.63159767 ... 587.69406409 833.40284741\n",
      "  613.44695901]\n",
      " [603.01423257 638.26175424 617.57867465 ... 609.85106122 613.44695901\n",
      "  803.3724249 ]] \n",
      "\n",
      "[[ 973.21404205  825.07636767  743.4229834  ...  757.76279181\n",
      "   814.35190281  739.49803341]\n",
      " [ 825.07636767 1098.12794627  784.653308   ...  857.67953162\n",
      "   864.36952811  776.62903896]\n",
      " [ 743.4229834   784.653308   1008.83785437 ...  843.04013998\n",
      "   817.6209166   718.30296049]\n",
      " ...\n",
      " [ 757.76279181  857.67953162  843.04013998 ... 1002.72035987\n",
      "   846.81564493  728.48260921]\n",
      " [ 814.35190281  864.36952811  817.6209166  ...  846.81564493\n",
      "  1176.19488062  766.30338737]\n",
      " [ 739.49803341  776.62903896  718.30296049 ...  728.48260921\n",
      "   766.30338737  859.34424114]] \n",
      "\n",
      "[[ 988.67374804  865.89857741  785.90750426 ...  799.03917408\n",
      "   859.48190519  774.82400479]\n",
      " [ 865.89857741 1115.57193543  830.08931392 ...  897.43286253\n",
      "   912.35228769  814.88297637]\n",
      " [ 785.90750426  830.08931392 1024.86345197 ...  879.23831295\n",
      "   864.30517239  756.92612958]\n",
      " ...\n",
      " [ 799.03917408  897.43286253  879.23831295 ... 1018.64877981\n",
      "   891.22143802  766.09481461]\n",
      " [ 859.48190519  912.35228769  864.30517239 ...  891.22143802\n",
      "  1194.87897914  808.64003793]\n",
      " [ 774.82400479  814.88297637  756.92612958 ...  766.09481461\n",
      "   808.64003793  872.99510184]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(true_sigma_hat,'\\n')\n",
    "print(sigma_hat,'\\n')\n",
    "print(np_sigma,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6201, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1304/3361954235.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_hat = torch.normal(torch.repeat_interleave(torch.tensor(mu), m), torch.repeat_interleave(torch.tensor(1/(batch_size-3)),m*concepts**2)).reshape(m, concepts**2)\n"
     ]
    }
   ],
   "source": [
    "c_hat, c_tilde = sample(num_concepts, num_latent_concepts, batch_size)\n",
    "c_hat = torch.tensor(c_hat)\n",
    "c_tilde = torch.tensor(c_tilde)\n",
    "batch_size = c_hat.shape[0]\n",
    "concepts = c.shape[1]\n",
    "c = torch.concatenate((c_hat, c_tilde), axis=1).squeeze() # 64, 256+128\n",
    "p_hat = torch.corrcoef(c.T) # 256+128, 256+128\n",
    "p_hat.fill_diagonal_(0.99999)\n",
    "\n",
    "mu = torch.arctanh(p_hat.flatten())\n",
    "z_hat = torch.normal(torch.repeat_interleave(torch.tensor(mu), m), torch.repeat_interleave(torch.tensor(1/(batch_size-3)),m*concepts**2)).reshape(m, concepts**2)\n",
    "# z_hat = np.random.standard_normal(m*concepts*concepts).reshape(m, concepts*concepts) * 1/(batch_size-3)**0.5 + torch.arctanh(p_hat.reshape(1,concepts*concepts))\n",
    "p_var = torch.var(torch.tanh(z_hat), axis=0)\n",
    "S_p = torch.sqrt(torch.sum(p_var))\n",
    "\n",
    "# estimate gamma\n",
    "for gamma in range(2, 10, 2):\n",
    "    p_hat_gamma = p_hat**gamma * p_hat\n",
    "    if torch.norm(p_hat - p_hat_gamma) >= delta * S_p:\n",
    "        gamma_star = gamma\n",
    "        break\n",
    "\n",
    "# estimate alpha\n",
    "scale = 1000\n",
    "step = 0.1\n",
    "for alpha in range(0, int((1+step)*scale), int(step*scale)):\n",
    "    alpha /= scale\n",
    "    L_alpha = alpha * p_hat ** gamma_star + (1-alpha) *p_hat ** (gamma_star-2)\n",
    "    p_hat_alpha = L_alpha * p_hat\n",
    "    if torch.norm(p_hat - p_hat_alpha) >= delta * S_p:\n",
    "        alpha_star = alpha - step\n",
    "\n",
    "L_alpha_star = alpha_star * p_hat ** gamma_star + (1-alpha_star) *p_hat ** (gamma_star-2)\n",
    "p_hat_nice = L_alpha_star * p_hat\n",
    "\n",
    "V_hat = torch.diagflat(torch.std(c, axis=0))\n",
    "P_hat_nice = torch.matmul(torch.matmul(V_hat, p_hat_nice), V_hat) # Covariance estimate\n",
    "\n",
    "print(-0.5 * torch.log(torch.linalg.det(P_hat_nice[-c_tilde.shape[1]:, -c_tilde.shape[1]:]) / torch.linalg.det(P_hat_nice)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1304/290463341.py:1: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  np.tanh(np.arctanh(1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(np.arctanh(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0],\n",
       "       [ 2,  3],\n",
       "       [ 8, 10],\n",
       "       [18, 21],\n",
       "       [32, 36]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10).reshape(5,2) * np.arange(5).reshape(5,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(c_cov_mean.flatten(), c_cov_std.flatten(), s=0.1)\n",
    "# plt.hist(c_cov.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.85758443 0.34428316 0.34187303 ... 0.30198384 0.3421407  0.31602216]\n",
      " [0.34428316 2.82694093 0.33168922 ... 0.35264139 0.33767396 0.34207855]\n",
      " [0.34187303 0.33168922 2.85041625 ... 0.33417808 0.34173658 0.31345031]\n",
      " ...\n",
      " [0.30198384 0.35264139 0.33417808 ... 2.8211648  0.34391872 0.34208158]\n",
      " [0.3421407  0.33767396 0.34173658 ... 0.34391872 2.82132786 0.33203742]\n",
      " [0.31602216 0.34207855 0.31345031 ... 0.34208158 0.33203742 2.85716297]]\n"
     ]
    }
   ],
   "source": [
    "print(c_cov_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(10).reshape(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
