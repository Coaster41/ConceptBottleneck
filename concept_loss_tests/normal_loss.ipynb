{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10**4\n",
    "num_concepts = 10\n",
    "num_latent_concepts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from independent normals using parameterization trick\n",
    "def sample(num_concepts, num_latent_concepts, batch_size):\n",
    "    mu_hat = np.random.random(num_concepts).reshape(num_concepts, 1) # (n_concepts,  1)\n",
    "    true_sigma_hat = np.random.random(num_concepts).reshape(num_concepts, 1) # (n_concepts,  1)\n",
    "    c_hat = np.random.standard_normal(batch_size*num_concepts).reshape(batch_size, num_concepts, 1) * true_sigma_hat**0.5 + mu_hat # (batch_size, n_concepts,  1)\n",
    "\n",
    "    mu_tilde = np.random.random(num_latent_concepts).reshape(num_latent_concepts, 1) # (n_latent,  1)\n",
    "    true_sigma_tilde = np.random.random(num_latent_concepts).reshape(num_latent_concepts, 1) # (n_latent,  1)\n",
    "    c_tilde = np.random.standard_normal(batch_size*num_latent_concepts).reshape(batch_size, num_latent_concepts, 1) * true_sigma_tilde**0.5 + mu_tilde # (batch_size, n_latent,  1)\n",
    "    # print(list(true_sigma_hat.squeeze())+list(true_sigma_tilde.squeeze()))\n",
    "    return c_hat, c_tilde\n",
    "\n",
    "# c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "# c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "# print(c_hat.shape, c_tilde.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "num_concepts = 10\n",
    "num_latent_concepts = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.52781254 -0.00342716 -0.13900249  0.22957528]\n",
      " [-0.00342716  0.07193949 -0.05867424  0.10551622]\n",
      " [-0.13900249 -0.05867424  0.4839676  -0.02610297]\n",
      " [ 0.22957528  0.10551622 -0.02610297  0.70553226]]\n",
      "\n",
      "[[ 0.52781254 -0.00342716 -0.13900249  0.22957528]\n",
      " [-0.00342716  0.07193949 -0.05867424  0.10551622]\n",
      " [-0.13900249 -0.05867424  0.4839676  -0.02610297]\n",
      " [ 0.22957528  0.10551622 -0.02610297  0.70553226]]\n"
     ]
    }
   ],
   "source": [
    "def covariance_short(c_hat, c_tilde):\n",
    "    c_hat = c_hat.squeeze(axis=-1).T\n",
    "    c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "    samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "    \n",
    "    samples_mean = np.mean(samples, axis=1)\n",
    "    exey = np.outer(samples_mean, samples_mean)\n",
    "    exy = np.zeros(exey.shape)\n",
    "    cov_again = np.zeros(exey.shape)\n",
    "    for i in range(exey.shape[0]):\n",
    "        mean_row_i = np.mean(samples[i])\n",
    "        sample_mean_row_i = np.sum(samples[i]) / (len(samples[i])-1)\n",
    "        for j in range(exey.shape[1]):\n",
    "            mean_row_j = np.mean(samples[j])\n",
    "            sample_mean_row_j = np.sum(samples[j]) / (len(samples[j])-1)\n",
    "            # cov_again[i,j] = np.mean(samples[i]*samples[j]) - mean_row_i*mean_row_j\n",
    "            cov_again[i,j] = np.sum((samples[i]-mean_row_i) * (samples[j]-mean_row_j)) / (len(samples[j])-1)\n",
    "            # cov_again[i,j] = np.sum( samples[i]*samples[j] - samples[i]*mean_row_j - mean_row_i*samples[j] + mean_row_i*mean_row_j ) / (len(samples[j])-1)\n",
    "            # cov_again[i,j] = np.sum( samples[i]*samples[j] ) / (len(samples[j])-1) - sample_mean_row_i*mean_row_j - mean_row_i*sample_mean_row_j + mean_row_i*mean_row_j\n",
    "            # cov_again[i,j] = np.sum(samples[i]*samples[j])/(samples.shape[1]) - mean_row_i*mean_row_j\n",
    "            exy[i,j] = np.mean(samples[i]*samples[j])\n",
    "    # print(exy.shape)\n",
    "    # exy_true = np.cov(samples) + exey\n",
    "\n",
    "    # print(exy)\n",
    "    # print()\n",
    "    # print(exy_true)\n",
    "\n",
    "\n",
    "    cov = exy - exey\n",
    "    return cov_again\n",
    "\n",
    "def covariance_np(c_hat, c_tilde):\n",
    "    c_hat = c_hat.squeeze(axis=-1).T\n",
    "    c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "    samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "    \n",
    "    return np.cov(samples)\n",
    "\n",
    "\n",
    "    # print(samples_mean.shape)\n",
    "    # cov = \n",
    "\n",
    "c_hat, c_tilde = sample(num_concepts, num_latent_concepts, batch_size)\n",
    "sigma = covariance(c_hat, c_tilde)\n",
    "sigma_np = covariance_np(c_hat, c_tilde)\n",
    "print(sigma)\n",
    "print()\n",
    "print(sigma_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(c_hat, c_tilde):\n",
    "    c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "    c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "    c_stack_T = np.concatenate((c_hat,c_tilde), axis=1) # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_hat_mean_resize = np.repeat(c_hat_mean.reshape(1,num_concepts,1), batch_size, axis=0) # (batch_size, n_concepts, 1)\n",
    "    c_tilde_mean_resize = np.repeat(c_tilde_mean.reshape(1,num_latent_concepts,1), batch_size, axis=0) # (batch_size, n_latent, 1)\n",
    "    c_stack_mean_T = np.concatenate((c_hat_mean_resize,c_tilde_mean_resize), axis=1)  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation = c_stack_T-c_stack_mean_T  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation_T = np.moveaxis(c_stack_deviation,-1,-2)  # (batch_size, 1, n_concepts+n_latent)\n",
    "    sigma = 1/(batch_size-1) * np.sum(np.matmul(c_stack_deviation,c_stack_deviation_T),axis=0)  # (batch_size, n_concepts+n_latent, n_concepts+n_latent)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8135731125215057, 0.7576167103399972, 0.31160523050571065, 0.8604086080584533]\n",
      "[[ 0.67459205 -0.415014    0.00300902 -0.02950988]\n",
      " [-0.415014    1.25191919 -0.17795501 -0.46840892]\n",
      " [ 0.00300902 -0.17795501  0.27022026  0.0027806 ]\n",
      " [-0.02950988 -0.46840892  0.0027806   0.86797543]]\n",
      "\n",
      "[[ 0.60713285 -0.3735126   0.00270811 -0.02655889]\n",
      " [-0.3735126   1.12672727 -0.16015951 -0.42156803]\n",
      " [ 0.00270811 -0.16015951  0.24319824  0.00250254]\n",
      " [-0.02655889 -0.42156803  0.00250254  0.78117789]]\n",
      "0.5258061794321258\n",
      "0.09648715386070177 0.06330522164800642\n"
     ]
    }
   ],
   "source": [
    "c_hat, c_tilde = sample(num_concepts, num_latent_concepts, batch_size)\n",
    "sigma_scratch = covariance(c_hat, c_tilde)\n",
    "c_hat = c_hat.squeeze(axis=-1).T\n",
    "c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "sigma_np = np.cov(samples)\n",
    "print(sigma_np)\n",
    "print()\n",
    "print(sigma_scratch)\n",
    "print(np.sum(np.abs(sigma_scratch - sigma_np)))\n",
    "print(np.linalg.det(sigma_np), np.linalg.det(sigma_scratch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_np(c_hat, c_tilde):\n",
    "    c_hat = c_hat.squeeze(axis=-1).T\n",
    "    c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "    samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "    sigma = np.cov(samples)\n",
    "    # print(sigma)\n",
    "    det_sigma = np.linalg.det(sigma)\n",
    "    # print(det_sigma)\n",
    "    log_det_sigma = np.log(det_sigma)\n",
    "\n",
    "    # print(samples[c_hat.shape[0]:, c_hat.shape[0]:].shape)\n",
    "    sigma_tilde = np.cov(samples[c_hat.shape[0]:, c_hat.shape[0]:])\n",
    "    det_sigma_tilde = np.linalg.det(sigma_tilde)\n",
    "    log_det_sigma_tilde = np.log(det_sigma_tilde) # scalar\n",
    "    r = (log_det_sigma - log_det_sigma_tilde) / 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(c_hat, c_tilde):\n",
    "    c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "    c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "    c_stack_T = np.concatenate((c_hat,c_tilde), axis=1) # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_hat_mean_resize = np.repeat(c_hat_mean.reshape(1,num_concepts,1), batch_size, axis=0) # (batch_size, n_concepts, 1)\n",
    "    c_tilde_mean_resize = np.repeat(c_tilde_mean.reshape(1,num_latent_concepts,1), batch_size, axis=0) # (batch_size, n_latent, 1)\n",
    "    c_stack_mean_T = np.concatenate((c_hat_mean_resize,c_tilde_mean_resize), axis=1)  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation = c_stack_T-c_stack_mean_T  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation_T = np.moveaxis(c_stack_deviation,-1,-2)  # (batch_size, 1, n_concepts+n_latent)\n",
    "    sigma = 1/(batch_size-1) * np.sum(np.matmul(c_stack_deviation,c_stack_deviation_T),axis=0)  # (batch_size, n_concepts+n_latent, n_concepts+n_latent)\n",
    "    return sigma\n",
    "\n",
    "def loss(c_hat, c_tilde, covariance):\n",
    "    # c_hat = c_hat.squeeze(axis=-1).T\n",
    "    # c_tilde = c_tilde.squeeze(axis=-1).T\n",
    "    # samples = np.concatenate((c_hat, c_tilde), axis=0)\n",
    "    sigma = covariance(c_hat, c_tilde)\n",
    "    # print(sigma)\n",
    "    det_sigma = np.linalg.det(sigma)\n",
    "    # print(det_sigma)\n",
    "    log_det_sigma = np.log(det_sigma)\n",
    "\n",
    "    # print(samples[c_hat.shape[0]:, c_hat.shape[0]:].shape)\n",
    "    sigma_tilde = sigma[c_hat.shape[1]:, c_hat.shape[1]:]\n",
    "    det_sigma_tilde = np.linalg.det(sigma_tilde)\n",
    "    log_det_sigma_tilde = np.log(det_sigma_tilde) # scalar\n",
    "    r = (log_det_sigma - log_det_sigma_tilde) / 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_term(c_hat, c_tilde):\n",
    "    c_hat_mean = np.mean(c_hat, axis=0) # (n_concepts, 1)\n",
    "    c_tilde_mean = np.mean(c_tilde, axis=0) # (n_latent, 1)\n",
    "\n",
    "    c_stack_T = np.concatenate((c_hat,c_tilde), axis=1) # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_hat_mean_resize = np.repeat(c_hat_mean.reshape(1,num_concepts,1), batch_size, axis=0) # (batch_size, n_concepts, 1)\n",
    "    c_tilde_mean_resize = np.repeat(c_tilde_mean.reshape(1,num_latent_concepts,1), batch_size, axis=0) # (batch_size, n_latent, 1)\n",
    "    c_stack_mean_T = np.concatenate((c_hat_mean_resize,c_tilde_mean_resize), axis=1)  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation = c_stack_T-c_stack_mean_T  # (batch_size, n_concepts+n_latent, 1)\n",
    "    c_stack_deviation_T = np.moveaxis(c_stack_deviation,-1,-2)  # (batch_size, 1, n_concepts+n_latent)\n",
    "    sigma = 1/(batch_size-1) * np.sum(np.matmul(c_stack_deviation,c_stack_deviation_T),axis=0)  # (batch_size, n_concepts+n_latent, n_concepts+n_latent)\n",
    "    # sigma = covariance(c_hat, c_tilde)\n",
    "    log_det_sigma = np.log(np.linalg.det(sigma)) # scalar\n",
    "\n",
    "    c_tilde_deviation = c_tilde-c_tilde_mean # (batch_size, n_latent, 1)\n",
    "    c_tilde_deviation_T = np.moveaxis(c_tilde_deviation,-1,-2) # (batch_size, 1, n_latent)\n",
    "    sigma_tilde = 1/(batch_size-1) * np.sum(np.matmul(c_tilde_deviation, c_tilde_deviation_T),axis=0)  # (n_latent, n_latent)\n",
    "    # sigma_tilde = sigma[c_hat.shape[1]:, c_hat.shape[1]:]\n",
    "    log_det_sigma_tilde = np.log(np.linalg.det(sigma_tilde)) # scalar\n",
    "\n",
    "    r = (log_det_sigma - log_det_sigma_tilde) / 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-41.084284197087044 -40.9086522377501\n",
      "-42.137530048934295 nan\n",
      "nan -40.39988889182447\n",
      "nan nan\n",
      "-inf nan\n",
      "-44.02085428672016 -43.56064762977157\n",
      "-40.58434335170526 nan\n",
      "-41.505314983439085 nan\n",
      "-46.32256784863933 -45.229031745703224\n",
      "-40.56238790110159 -41.06579027080908\n",
      "nan nan\n",
      "-43.68012419514183 nan\n",
      "nan nan\n",
      "-inf nan\n",
      "-42.533654837691 -42.5277376088675\n",
      "-43.05203536061653 nan\n",
      "-42.88556288283602 -42.68247294166345\n",
      "-45.19056968665869 nan\n",
      "-46.35851228733922 nan\n",
      "-44.730288835756056 -43.952192136798416\n",
      "nan nan\n",
      "nan nan\n",
      "-44.46914255509125 -45.080475023760314\n",
      "nan nan\n",
      "nan nan\n",
      "-41.96386476976567 -42.66650924606349\n",
      "nan -43.53196394342383\n",
      "nan nan\n",
      "-44.49876164274746 nan\n",
      "nan nan\n",
      "nan nan\n",
      "-41.373704241657286 nan\n",
      "nan -45.43108380180514\n",
      "nan -41.68178148305259\n",
      "nan -44.08505439149189\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "-42.3491365378795 nan\n",
      "nan -41.5042444151433\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "-43.83986429296105 -43.38866967164441\n",
      "-43.85107627150666 -44.29383580811036\n",
      "-inf nan\n",
      "nan -41.59788245355249\n",
      "-40.98504187398684 -inf\n",
      "-39.77514334317138 nan\n",
      "nan nan\n",
      "nan nan\n",
      "-44.786622111065896 -44.65427583395235\n",
      "-45.36127846223246 -46.06662136935449\n",
      "nan nan\n",
      "nan -42.232643804297176\n",
      "-43.30032425283807 nan\n",
      "nan -46.94063891449224\n",
      "nan nan\n",
      "nan -38.62238279523215\n",
      "nan -42.49983456524011\n",
      "nan -42.853825257611824\n",
      "-42.321901169463274 -41.81849879975577\n",
      "-inf -40.10378819872962\n",
      "nan nan\n",
      "-44.20031057428619 nan\n",
      "nan nan\n",
      "-45.9754337182882 -46.076118594170524\n",
      "nan nan\n",
      "-42.03879803125943 nan\n",
      "nan -45.358584135031876\n",
      "nan -42.179458960588576\n",
      "nan nan\n",
      "-42.65314468636411 -inf\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "-43.29972383352181 -42.06651622390425\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "-40.2306355551011 -40.70599455112272\n",
      "nan nan\n",
      "-41.575002310157245 nan\n",
      "nan nan\n",
      "nan -40.982103513539215\n",
      "-42.97341196239523 nan\n",
      "-42.07992334812655 -41.42208495117358\n",
      "nan nan\n",
      "-42.79233780435312 nan\n",
      "-41.336969277743584 -40.73934971556499\n",
      "-42.222522960914645 -40.980069636020644\n",
      "-41.858460188906385 -41.14091792626173\n",
      "-43.067343779305595 nan\n",
      "-42.7139496910779 nan\n",
      "-39.96032604035084 nan\n",
      "-43.070947960385624 nan\n",
      "nan -44.700423260026525\n",
      "nan -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1040/3403182430.py:13: RuntimeWarning: invalid value encountered in log\n",
      "  log_det_sigma = np.log(np.linalg.det(sigma)) # scalar\n",
      "/tmp/ipykernel_1040/3746537734.py:21: RuntimeWarning: invalid value encountered in log\n",
      "  log_det_sigma = np.log(det_sigma)\n",
      "/tmp/ipykernel_1040/3746537734.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "  log_det_sigma = np.log(det_sigma)\n",
      "/tmp/ipykernel_1040/3403182430.py:13: RuntimeWarning: divide by zero encountered in log\n",
      "  log_det_sigma = np.log(np.linalg.det(sigma)) # scalar\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "num_concepts = 7\n",
    "num_latent_concepts = 4\n",
    "\n",
    "for _ in range(100):\n",
    "    c_hat, c_tilde = sample(num_concepts, num_latent_concepts, batch_size)\n",
    "    # r = loss_term(c_hat, c_tilde)\n",
    "    # r_fast = loss_np(c_hat, c_tilde)\n",
    "    r = loss(c_hat, c_tilde, covariance_np)\n",
    "    # r_fast = loss(c_hat, c_tilde, covariance)\n",
    "    r_fast = loss_term(c_hat, c_tilde)\n",
    "    # print(r_fast)\n",
    "    # if np.isnan(r_fast):\n",
    "    #     break\n",
    "    print(r, r_fast)\n",
    "    # print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5875920078389916\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchcfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
